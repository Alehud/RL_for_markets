{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import doubleauction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleauction.util import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "theta = 0.7\n",
    "sigma = 15.\n",
    "p = OrnsteinUhlenbeckProcess(theta=theta, sigma = sigma)\n",
    "sigma * sigma / 2 / theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(100):\n",
    "    l.append(p.sample())\n",
    "    \n",
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleauction\n",
    "from doubleauction.agents import RandomSeller, RandomBuyer, DDPGSellerOU\n",
    "from doubleauction.environments import MarketEnvironment\n",
    "\n",
    "from doubleauction.matchers import RandomMatcher\n",
    "\n",
    "from doubleauction.util import SequentialMemory, hard_update, soft_update\n",
    "from doubleauction.util import generate_seller_prices_paper, generate_buyer_prices_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "records['rewards'] = []\n",
    "records['demands'] = []\n",
    "records['prices'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "epochs = 500\n",
    "warmup_epochs = 20\n",
    "seller_agent = DDPGSellerOU('learner', 0, \n",
    "                                ou_theta=.7, ou_mu=.0, ou_sigma=15., sigma_min=3.5, anneal_steps=300*10*10,\n",
    "                                  discount = 0.97, lr = 3e-4, \n",
    "                                  wd = 1e-4, mem_size=500000, tau=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdict = torch.load('results/models2')\n",
    "seller_agent.actor.load_state_dict(mdict['actor'])\n",
    "seller_agent.actor_target.load_state_dict(mdict['actor_target'])\n",
    "\n",
    "seller_agent.critic.load_state_dict(mdict['critic'])\n",
    "seller_agent.critic_target.load_state_dict(mdict['critic_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for e in range(epochs):\n",
    "    seller_agent.reservation_price = generate_seller_prices_paper(1)[0]\n",
    "    \n",
    "    sellers = []\n",
    "    for ii, p in enumerate(generate_seller_prices_paper(19)):\n",
    "        sellers.append(RandomSeller('s'+str(ii), p))\n",
    "    sellers.append(seller_agent)\n",
    "\n",
    "    buyers = []\n",
    "    for ii, p in enumerate(generate_buyer_prices_paper(20)):\n",
    "        buyers.append(RandomBuyer('b'+str(ii), p))\n",
    "\n",
    "    agents = sellers + buyers\n",
    "    \n",
    "    seller_agent.new_game()\n",
    "    \n",
    "#     setting = {\n",
    "#         'self_last_offer': False,\n",
    "#         'same_side_last_offers': False,\n",
    "#         'other_side_last_offers': False,\n",
    "#         'completed_deals': False,\n",
    "#         'current_time': False,\n",
    "#         'max_time': False,\n",
    "#         'n_sellers': False,\n",
    "#         'n_buyers': False\n",
    "#     }\n",
    "    \n",
    "    ROUNDS_PER_GAME = 10\n",
    "\n",
    "    market_env = MarketEnvironment(sellers=sellers, buyers=buyers, max_time=10, ## not the same as rounds per game!!\n",
    "                                   matcher=RandomMatcher(reward_on_reference=True))\n",
    "    init_observation = market_env.reset()\n",
    "\n",
    "    round_avg = 0.\n",
    "    offer_avg = 0.\n",
    "    time_avg = 0.\n",
    "    \n",
    "    records['demands'].append([])\n",
    "    records['rewards'].append([])\n",
    "    records['prices'].append(seller_agent.reservation_price)\n",
    "\n",
    "    for n_round in range(10):\n",
    "        \n",
    "        init_observation = market_env.reset()\n",
    "        observations = {k.agent_id:None for k in agents}\n",
    "        done = {k.agent_id:False for k in agents}\n",
    "        reward_hist = []\n",
    "        rounds = 0\n",
    "        terminate_round = False\n",
    "        \n",
    "        seller_agent.new_round()\n",
    "        \n",
    "        records['demands'][-1].append([])\n",
    "        records['rewards'][-1].append([])\n",
    "        \n",
    "        offers_list = []\n",
    "        \n",
    "        while not terminate_round:\n",
    "            offers = {}\n",
    "\n",
    "            for iagent in agents:\n",
    "                iagent.receive_observations_from_environment(market_env)\n",
    "                \n",
    "            offers = {a.agent_id : a.decide() for a in agents}\n",
    "\n",
    "            market_env.step(offers)\n",
    "            \n",
    "            rewards = market_env.rewards\n",
    "            \n",
    "            reward_hist.append(rewards)\n",
    "            rounds += 1\n",
    "\n",
    "            terminate_round = market_env.if_round_done or \\\n",
    "                                market_env.agents[market_env.agents['id'] == 'learner']['done'].iloc[0]\n",
    "\n",
    "            # create record of experience\n",
    "            seller_agent.memorize(rewards['learner'], terminate_round)\n",
    "            \n",
    "            offers_list.append(offers['learner'] - seller_agent.reservation_price)\n",
    "            \n",
    "            records['demands'][-1][-1].append(offers['learner'] - seller_agent.reservation_price)\n",
    "            records['rewards'][-1][-1].append(rewards['learner'])\n",
    "            \n",
    "            round_avg += rewards['learner']\n",
    "\n",
    "            time_avg += 1\n",
    "    \n",
    "        offer_avg += sum(offers_list) / len(offers_list)\n",
    "#         time_vs_rewards.append(round_avg)\n",
    "#         time_vs_demand.append(sum(offers_list) / len(offers_list))\n",
    "        \n",
    "        if e >= warmup_epochs:\n",
    "            seller_agent.learn()\n",
    "    \n",
    "    print('Epoch: {}, Avg. earnings: {}, Avg. demand: {}, Avg. time: {}'.format(e, round_avg / 10., \n",
    "                                                                            offer_avg / 10.,\n",
    "                                                                            time_avg / 10.))\n",
    "    \n",
    "    if (e + 1) % 100 == 0:\n",
    "        torch.save({'actor':seller_agent.actor.state_dict(),\n",
    "                   'actor_target':seller_agent.actor_target.state_dict(),\n",
    "                  'critic':seller_agent.critic.state_dict(),\n",
    "                  'critic_target':seller_agent.critic_target.state_dict()}, 'results/models_ou1_e{}'.format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "l2 = flatten( records['rewards'] )\n",
    "l3 = [sum(ll) for ll in l2]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(l3)\n",
    "# plt.plot(smooth(l2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(records, 'results/records_ou1')\n",
    "\n",
    "torch.save({'actor':seller_agent.actor.state_dict(),\n",
    "           'actor_target':seller_agent.actor_target.state_dict(),\n",
    "          'critic':seller_agent.critic.state_dict(),\n",
    "          'critic_target':seller_agent.critic_target.state_dict()}, 'results/models_ou1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seller_agent.memory, 'results/memory_ou1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
